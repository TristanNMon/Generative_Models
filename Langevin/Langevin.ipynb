{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Neural Denoisers\n",
    "\n",
    "\n",
    "\n",
    " This notebook is a reproduction of some of the results presented in the paper [Deep Image Priors, Ulyanov 2018](https://arxiv.org/abs/1711.10925).\n",
    " \n",
    "\n",
    " What we will do is take an image $x_0$ from the internet and use a Neural Network (UNet) to solve\n",
    "\n",
    " $$\n",
    "\n",
    "     \\theta \\in \\argmin \\|D_{\\theta}(x_{\\sigma}) - x_\\sigma \\|^2 \\;,\n",
    "\n",
    " $$\n",
    "\n",
    " where $D_{\\theta}$ is a convolutional neural net and $x_\\sigma = x_0 + \\sigma z$ where $z$ is a realization of a standard Gaussian noise.\n",
    "\n",
    " More concretely, we will follow the quantity $ L_0(\\theta_t) = \\|D_{\\theta_t}(x_{in}) - x_0 \\|^2$ and $L_{\\sigma}(\\theta_t) = \\|D_{\\theta_t}(x_{in}) - x_\\sigma \\|^2$ through the optimization trajectory $\\theta_{1:n}$ of the optimization problem. For $x_{in}$, we can chose both $x_\\sigma$ or a random (fixed during training) input!\n",
    "\n",
    " ### Quick PyTorch Tutorial\n",
    "\n",
    " This is a **really fast** pytorch tutorial that introduces the basic pytorch functions that you will need.\n",
    "\n",
    " 1. **The basic neural network class**: A neural Network is an instance a class that inherits from the basic `torch.nn.Module` class. You can see how it was made in the class `UnetWithoutSkip` below. The important thing for us is that it contains a bunch of **parameters** which are the quantities that we are going to optimize. It also implements a `forward` function, which is responsible for taking the input and producing the output.\n",
    "\n",
    " 2. **Calculating gradients of the network weights**: The first thing we do is ask torch to \"track\" the propagation (the composition) of the parameters of the network for a certain operation. This is done by setting `mymodel.requires_grad_(True)`. Then, we can calculate any quantity that uses `mymodel` and then calculate the gradient of the parameters with respect to this quantity. Below is a dumb example\n",
    "\n",
    "    ```\n",
    "\n",
    "    x_out = mymodel(x_in)\n",
    "\n",
    "    loss = (x_out**2).sum()\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    ```\n",
    "\n",
    " The code above calculates the gradients of the parameters of `mymodel`with respect to the loss defined (the squared sum of the outputs).\n",
    "\n",
    " 3. **Optimizing**: While it is possible to manually write a gradient descent, by iterating through each element of `mymodel.parameters()` and updating it using the attribute `.grad` of each model, the classes from `torch.optim` handle all this for us. There are several available options, but we will use the Adam optimizer.\n",
    "\n",
    " To do so, the first thing one needs to do is to instanciate it with the parameters we need to optimize:\n",
    "\n",
    "     ```optimizer = torch.optim.Adam(mymodel.parameters(), lr=1e-2)```\n",
    "\n",
    " Then, two methods are really useful:\n",
    "\n",
    " ```optimizer.zero_grad()``` -> Cleans the gradients, so we can start a new gradient computation\n",
    "\n",
    " ```optimizer.step()``` -> Updates the value of the variables according to a gradient that has been calculated (by YOU!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import (\n",
    "    ToPILImage,\n",
    "    ToTensor,\n",
    "    Normalize,\n",
    "    Compose,\n",
    ")\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import PIL\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import platform\n",
    "from matplotlib import animation\n",
    "from matplotlib import rc\n",
    "from typing import Callable\n",
    "from functools import partial\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "rc(\"animation\", html=\"jshtml\")\n",
    "\n",
    "\n",
    "def get_device():\n",
    "    # Check if CUDA is available\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        print(f\"Using {device} for computation.\")\n",
    "        return device\n",
    "    elif (\n",
    "        \"Apple\" in platform.system()\n",
    "    ):  # Check for Apple Silicon devices (MacOS on M1, etc.)\n",
    "        device = torch.device(\"mps\")  # Use Apple Metal Performance Shaders\n",
    "        print(f\"Using {device} for computation.\")\n",
    "        return device\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(f\"Using CPU for computation.\")\n",
    "        return device\n",
    "\n",
    "\n",
    "# Get the device\n",
    "device = get_device()\n",
    "\n",
    "\n",
    "def image_grid(imgs, rows, cols):\n",
    "\n",
    "    w, h = imgs[0].size\n",
    "    grid = PIL.Image.new(\"RGB\", size=(cols * w, rows * h))\n",
    "    grid_w, grid_h = grid.size\n",
    "\n",
    "    for i, (img, _) in enumerate(zip(imgs, range(cols * rows))):\n",
    "        grid.paste(img, box=(i % cols * w, i // cols * h))\n",
    "    return grid\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.manual_seed(42)\n",
    "url = \"https://cdn.pixabay.com/photo/2024/08/15/19/19/highland-cow-8972000_960_720.jpg\"\n",
    "img = PIL.Image.open(requests.get(url, stream=True).raw).crop((194, 88, 706, 600))\n",
    "\n",
    "print(img.size)\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell describes transforming image to [-1, 1] data and back\n",
    "img2data = Compose([ToTensor(), Normalize([0.5], [0.5])])\n",
    "data2img = Compose([Normalize([-1.0], [2.0]), ToPILImage()])\n",
    "\n",
    "img_data = img2data(img)  # Transform img to [-1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell creates a noisy version\n",
    "sigma = 0.5\n",
    "noise = torch.randn_like(img_data)\n",
    "noised_img_data = img_data + sigma * noise\n",
    "noised_img = data2img(noised_img_data)\n",
    "noised_img\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network related stuff, taken and adapted from https://github.com/NVlabs/edm2/tree/main\n",
    "def constant(value, shape=None, dtype=None, device=None, memory_format=None):\n",
    "    value = np.asarray(value)\n",
    "    if shape is not None:\n",
    "        shape = tuple(shape)\n",
    "    if dtype is None:\n",
    "        dtype = torch.get_default_dtype()\n",
    "    if device is None:\n",
    "        device = torch.device(\"cpu\")\n",
    "    if memory_format is None:\n",
    "        memory_format = torch.contiguous_format\n",
    "\n",
    "    tensor = torch.as_tensor(value.copy(), dtype=dtype, device=device)\n",
    "    if shape is not None:\n",
    "        tensor, _ = torch.broadcast_tensors(tensor, torch.empty(shape))\n",
    "    tensor = tensor.contiguous(memory_format=memory_format)\n",
    "\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def const_like(ref, value, shape=None, dtype=None, device=None, memory_format=None):\n",
    "    if dtype is None:\n",
    "        dtype = ref.dtype\n",
    "    if device is None:\n",
    "        device = ref.device\n",
    "    return constant(\n",
    "        value, shape=shape, dtype=dtype, device=device, memory_format=memory_format\n",
    "    )\n",
    "\n",
    "\n",
    "def normalize(x, dim=None, tol=1e-4):\n",
    "    if dim is None:\n",
    "        dim = list(range(1, x.ndim))\n",
    "    norm = torch.linalg.vector_norm(x, dim=dim, keepdim=True)\n",
    "    norm = norm + tol * (norm.numel() / x.numel()) ** 0.5\n",
    "    return x  # / norm Deactivate the normalization\n",
    "\n",
    "\n",
    "def resample(x, f=[1, 1], mode=\"keep\"):\n",
    "    if mode == \"keep\":\n",
    "        return x\n",
    "    f = np.float32(f)\n",
    "    assert f.ndim == 1 and len(f) % 2 == 0\n",
    "    pad = (len(f) - 1) // 2\n",
    "    f = f / f.sum()\n",
    "    f = np.outer(f, f)[np.newaxis, np.newaxis, :, :]\n",
    "    f = const_like(x, f)\n",
    "    c = x.shape[1]\n",
    "    if mode == \"down\":\n",
    "        return torch.nn.functional.conv2d(\n",
    "            x, f.tile([c, 1, 1, 1]), groups=c, stride=2, padding=(pad,)\n",
    "        )\n",
    "    assert mode == \"up\"\n",
    "    return torch.nn.functional.conv_transpose2d(\n",
    "        x, (f * 4).tile([c, 1, 1, 1]), groups=c, stride=2, padding=(pad,)\n",
    "    )\n",
    "\n",
    "\n",
    "def mp_silu(x):\n",
    "    return torch.nn.functional.silu(x) / 0.596\n",
    "\n",
    "\n",
    "def mp_sum(a, b, t=0.5):\n",
    "    return a.lerp(b, t) / np.sqrt((1 - t) ** 2 + t**2)\n",
    "\n",
    "\n",
    "class MPConv(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel):\n",
    "        super().__init__()\n",
    "        self.out_channels = out_channels\n",
    "        self.weight = torch.nn.Parameter(\n",
    "            torch.randn(out_channels, in_channels, *kernel)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, gain=1):\n",
    "        w = self.weight.to(torch.float32)\n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                self.weight.copy_(normalize(w))  # forced weight normalization\n",
    "        w = normalize(w)  # traditional weight normalization\n",
    "        w = w * (gain / np.sqrt(w[0].numel()))  # magnitude-preserving scaling\n",
    "        w = w.to(x.dtype)\n",
    "        if w.ndim == 2:\n",
    "            return x @ w.t()\n",
    "        assert w.ndim == 4\n",
    "        return torch.nn.functional.conv2d(x, w, padding=(w.shape[-1] // 2,))\n",
    "\n",
    "\n",
    "class Block(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,  # Number of input channels.\n",
    "        out_channels,  # Number of output channels.\n",
    "        flavor=\"enc\",  # Flavor: 'enc' or 'dec'.\n",
    "        resample_mode=\"keep\",  # Resampling: 'keep', 'up', or 'down'.\n",
    "        resample_filter=[1, 1],  # Resampling filter.\n",
    "        dropout=0,  # Dropout probability.\n",
    "        res_balance=0.3,  # Balance between main branch (0) and residual branch (1).\n",
    "        clip_act=256,  # Clip output activations. None = do not clip.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.out_channels = out_channels\n",
    "        self.flavor = flavor\n",
    "        self.resample_filter = resample_filter\n",
    "        self.resample_mode = resample_mode\n",
    "        self.dropout = dropout\n",
    "        self.res_balance = res_balance\n",
    "        self.clip_act = clip_act\n",
    "        self.emb_gain = torch.nn.Parameter(torch.zeros([]))\n",
    "        self.conv_res0 = MPConv(\n",
    "            out_channels if flavor == \"enc\" else in_channels,\n",
    "            out_channels,\n",
    "            kernel=[3, 3],\n",
    "        )\n",
    "        self.conv_res1 = MPConv(out_channels, out_channels, kernel=[3, 3])\n",
    "        self.conv_skip = (\n",
    "            MPConv(in_channels, out_channels, kernel=[1, 1])\n",
    "            if in_channels != out_channels\n",
    "            else None\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Main branch.\n",
    "        x = resample(x, f=self.resample_filter, mode=self.resample_mode)\n",
    "        if self.flavor == \"enc\":\n",
    "            if self.conv_skip is not None:\n",
    "                x = self.conv_skip(x)\n",
    "            x = normalize(x, dim=1)  # pixel norm\n",
    "\n",
    "        # Residual branch.\n",
    "        y = self.conv_res0(mp_silu(x))\n",
    "        if self.training and self.dropout != 0:\n",
    "            y = torch.nn.functional.dropout(y, p=self.dropout)\n",
    "        y = self.conv_res1(y)\n",
    "\n",
    "        # Connect the branches.\n",
    "        if self.flavor == \"dec\" and self.conv_skip is not None:\n",
    "            x = self.conv_skip(x)\n",
    "        x = mp_sum(x, y, t=self.res_balance)\n",
    "\n",
    "        # Clip activations.\n",
    "        if self.clip_act is not None:\n",
    "            x = x.clip_(-self.clip_act, self.clip_act)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UNetWithoutSkip(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_resolution,  # Image resolution.\n",
    "        img_channels,  # Image channels.\n",
    "        model_channels=192,  # Base multiplier for the number of channels.\n",
    "        channel_mult=[\n",
    "            1,\n",
    "            2,\n",
    "            3,\n",
    "            4,\n",
    "        ],  # Per-resolution multipliers for the number of channels.\n",
    "        num_blocks=3,  # Number of residual blocks per resolution.\n",
    "        **block_kwargs,  # Arguments for Block.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        cblock = [model_channels * x for x in channel_mult]\n",
    "        self.out_gain = torch.nn.Parameter(torch.zeros([]))\n",
    "\n",
    "        # Encoder.\n",
    "        self.enc = torch.nn.ModuleDict()\n",
    "        cout = img_channels + 1\n",
    "        for level, channels in enumerate(cblock):\n",
    "            res = img_resolution >> level\n",
    "            if level == 0:\n",
    "                cin = cout\n",
    "                cout = channels\n",
    "                self.enc[f\"{res}x{res}_conv\"] = MPConv(cin, cout, kernel=[3, 3])\n",
    "            else:\n",
    "                self.enc[f\"{res}x{res}_down\"] = Block(\n",
    "                    cout, cout, flavor=\"enc\", resample_mode=\"down\", **block_kwargs\n",
    "                )\n",
    "            for idx in range(num_blocks):\n",
    "                cin = cout\n",
    "                cout = channels\n",
    "                self.enc[f\"{res}x{res}_block{idx}\"] = Block(\n",
    "                    cin, cout, flavor=\"enc\", **block_kwargs\n",
    "                )\n",
    "\n",
    "        # Decoder.\n",
    "        self.dec = torch.nn.ModuleDict()\n",
    "        for level, channels in reversed(list(enumerate(cblock))):\n",
    "            res = img_resolution >> level\n",
    "            if level == len(cblock) - 1:\n",
    "                self.dec[f\"{res}x{res}_in0\"] = Block(\n",
    "                    cout, cout, flavor=\"dec\", **block_kwargs\n",
    "                )\n",
    "                self.dec[f\"{res}x{res}_in1\"] = Block(\n",
    "                    cout, cout, flavor=\"dec\", **block_kwargs\n",
    "                )\n",
    "            else:\n",
    "                self.dec[f\"{res}x{res}_up\"] = Block(\n",
    "                    cout, cout, flavor=\"dec\", resample_mode=\"up\", **block_kwargs\n",
    "                )\n",
    "            for idx in range(num_blocks + 1):\n",
    "                cin = cout\n",
    "                cout = channels\n",
    "                self.dec[f\"{res}x{res}_block{idx}\"] = Block(\n",
    "                    cin, cout, flavor=\"dec\", **block_kwargs\n",
    "                )\n",
    "        self.out_conv = MPConv(cout, img_channels, kernel=[3, 3])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder.\n",
    "        x = torch.cat([x, torch.ones_like(x[:, :1])], dim=1)\n",
    "        for name, block in self.enc.items():\n",
    "            x = block(x)\n",
    "\n",
    "        # Decoder.\n",
    "        for name, block in self.dec.items():\n",
    "            x = block(x)\n",
    "        x = self.out_conv(x, gain=self.out_gain)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = torch.compile(\n",
    "    UNetWithoutSkip(\n",
    "        img_resolution=512,\n",
    "        img_channels=3,\n",
    "        model_channels=16,\n",
    "        channel_mult=[1, 1, 2],\n",
    "        num_blocks=2,\n",
    "    )\n",
    ").to(device)\n",
    "unet = unet.requires_grad_(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_steps = 4000\n",
    "lr = 1e-2\n",
    "stats = {\"loss2real\": [], \"loss2noised\": [], \"recImage\": []}\n",
    "optimizer = torch.optim.Adam(unet.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_data = noised_img_data[None].to(device)\n",
    "noised_img_data = noised_img_data.to(device)\n",
    "img_data = img_data.to(device)\n",
    "pbar = tqdm(range(N_steps))\n",
    "for step in pbar:\n",
    "    # Implement here your solution\n",
    "    optimizer.zero_grad()\n",
    "    pred = unet(in_data)[0]\n",
    "    loss2noised = ((noised_img_data - pred) ** 2).mean()\n",
    "    loss2noised.backward()\n",
    "    optimizer.step()\n",
    "    with torch.no_grad():\n",
    "        loss2real = ((img_data - pred) ** 2).mean()\n",
    "    if step % 10 == 0:\n",
    "        stats[\"loss2real\"].append(loss2real.item())\n",
    "        stats[\"loss2noised\"].append(loss2noised.item())\n",
    "        stats[\"recImage\"].append(data2img(pred))\n",
    "        pbar.set_postfix(\n",
    "            {\"loss noisy\": loss2noised.item(), \"loss clean\": loss2real.item()}\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(3, 3))\n",
    "\n",
    "ax.plot(stats[\"loss2real\"], label=r\"$L_{0}(\\theta_t)$\")\n",
    "ax.plot(stats[\"loss2noised\"], label=r\"$L_{\\sigma}(\\theta_t)$\")\n",
    "ax.set_xlabel(\"t\")\n",
    "ax.legend()\n",
    "fig.show()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 10))\n",
    "axes[0].imshow(image_grid(stats[\"recImage\"][-64:], rows=8, cols=8))\n",
    "axes[1].imshow(\n",
    "    image_grid(\n",
    "        [\n",
    "            img,\n",
    "            noised_img,\n",
    "            stats[\"recImage\"][np.argmin(stats[\"loss2real\"])],\n",
    "            stats[\"recImage\"][np.argmin(stats[\"loss2noised\"])],\n",
    "        ],\n",
    "        rows=2,\n",
    "        cols=2,\n",
    "    )\n",
    ")\n",
    "for ax in axes:\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langevin sampling with denoisers\n",
    "\n",
    "## Implementation a Langevin sampler for a given noise level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoiser_fn(x, sigma, class_labels):\n",
    "    with torch.no_grad():\n",
    "        pred = (\n",
    "              net(\n",
    "                x.to(device),\n",
    "                torch.ones(x.shape[0], device=device)*sigma,\n",
    "                class_labels=torch.nn.functional.one_hot(class_labels.long().to(device), 1000),\n",
    "            ).cpu()\n",
    "        )\n",
    "    return pred.clamp(-1, 1)\n",
    "\n",
    "\n",
    "def load_karras_net():\n",
    "    curr_dir = os.getcwd()\n",
    "    os.chdir(\"edm2\")\n",
    "    import dnnlib\n",
    "    path_to_model = 'https://nvlabs-fi-cdn.nvidia.com/edm2/posthoc-reconstructions/edm2-img64-s-1073741-0.075.pkl'\n",
    "    with dnnlib.util.open_url(path_to_model, verbose=True) as f:\n",
    "        data = pickle.load(f)\n",
    "    net = data['ema'].to(device)\n",
    "    os.chdir(curr_dir)\n",
    "    return torch.compile(net)\n",
    "net = load_karras_net()\n",
    "sigma_data = 0.5\n",
    "\n",
    "def run_langevin(\n",
    "    initial_samples: torch.Tensor,\n",
    "    denoiser_fn: Callable[[torch.Tensor, float], torch.Tensor],\n",
    "    sigma: float,\n",
    "    n_steps: int,\n",
    "    lr: float,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    initial_samples: Initialization for the Langevin algorithm\n",
    "    denoiser_fn: Function D(x, \\sigma), which gives the denoising for a given sigma.\n",
    "    n_steps: The number of steps to run Langevin.\n",
    "    sigma: Amount of noise\n",
    "    lr: The Langevin learning rate.\n",
    "    \"\"\"\n",
    "    # implement your code here\n",
    "    samples = initial_samples.clone()\n",
    "    pbar = tqdm(range(n_steps), desc=f\"Langevin {sigma:.2f}\")\n",
    "    for i in pbar:\n",
    "        score = (denoiser_fn(samples, sigma) - samples) / (sigma**2)\n",
    "        drift = score\n",
    "        samples = samples + lr * drift + ((2 * lr) ** 0.5) * torch.randn_like(samples)\n",
    "        if i % 10:\n",
    "            pbar.set_postfix(\n",
    "                {\n",
    "                    \"score norm\": torch.linalg.vector_norm(score[0]).item(),\n",
    "                    \"dinit\": torch.linalg.vector_norm(\n",
    "                        samples[0] - initial_samples[0]\n",
    "                    ).item(),\n",
    "                }\n",
    "            )\n",
    "    return samples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 2\n",
    "batch_size = 2\n",
    "n_langevin = 100\n",
    "lr = 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_samples = torch.randn((batch_size, 3, 64, 64)) * (\n",
    "    (sigma**2 + sigma_data**2) ** 0.5\n",
    ")\n",
    "class_labels = (\n",
    "    torch.ones(\n",
    "        batch_size,\n",
    "    )\n",
    "    * 240\n",
    ")\n",
    "samples = initial_samples.clone()\n",
    "\n",
    "denoiser_fixed_class_fn = partial(denoiser_fn, class_labels=class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = run_langevin(\n",
    "    initial_samples=samples,\n",
    "    denoiser_fn=denoiser_fixed_class_fn,\n",
    "    sigma=sigma,\n",
    "    lr=lr,\n",
    "    n_steps=n_langevin,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.imshow(\n",
    "    image_grid(\n",
    "        [\n",
    "            data2img(i)\n",
    "            for i in denoiser_fn(\n",
    "                torch.cat((initial_samples, samples)), sigma, class_labels.repeat(2)\n",
    "            )\n",
    "        ],\n",
    "        rows=2,\n",
    "        cols=2,\n",
    "    )\n",
    ")\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
